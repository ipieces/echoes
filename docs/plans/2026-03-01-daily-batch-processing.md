# Audio Journal â€” æ—¥çº§æ‰¹å¤„ç†æ”¹é€ æ–¹æ¡ˆ

> åŸºäºç°æœ‰ Phase 1 MVP ä»£ç ï¼Œå°†"å•æ–‡ä»¶å®æ—¶å¤„ç†"æ”¹ä¸º"æ—¥çº§æ‰¹å¤„ç†"æ¨¡å¼ã€‚

## èƒŒæ™¯

å½•éŸ³è®¾å¤‡æ¯å¤©äº§ç”Ÿè‹¥å¹² WAV æ–‡ä»¶ï¼Œå‘½åè§„åˆ™ `YYYYMMDDHHMMSS.WAV`ï¼ˆå¦‚ `20260301120546.WAV`ï¼‰ã€‚å½“å‰æ¶æ„é€æ–‡ä»¶ç‹¬ç«‹å¤„ç†ï¼Œå­˜åœ¨ä¸‰ä¸ªé—®é¢˜ï¼š

1. **è¯´è¯äºº ID å‰²è£‚** â€” æ–‡ä»¶é—´ SPEAKER_00 æ— æ³•å…³è”
2. **åœºæ™¯æ–­è£‚** â€” åŒä¸€ä¼šè®®è·¨ä¸¤ä¸ªæ–‡ä»¶ä¼šè¢«æ‹†æˆä¸¤ä¸ªç‹¬ç«‹åˆ†æ
3. **å½’æ¡£ç¢ç‰‡åŒ–** â€” ç¼ºå°‘æ—¥çº§å…¨æ™¯è§†å›¾

## æ ¸å¿ƒæ”¹åŠ¨

### 1. æ–°å¢ DailyBatchProcessor

**ä½ç½®**: `src/audio_journal/batch.py`

**èŒè´£**: æŒ‰æ—¥æœŸæ”¶é›†æ–‡ä»¶ â†’ æŒ‰æ—¶é—´æ’åº â†’ é¡ºåºå¤„ç† â†’ è·¨æ–‡ä»¶åœºæ™¯åˆå¹¶ â†’ ç”Ÿæˆæ—¥æŠ¥

```python
class DailyBatchProcessor:
    """æ—¥çº§æ‰¹å¤„ç†å™¨ã€‚"""

    def __init__(self, config: AppConfig, pipeline: Pipeline):
        self.config = config
        self.pipeline = pipeline
        self.scene_merger = SceneMerger(config)

    async def process_day(self, date: str, files: list[Path]) -> DailyReport:
        """å¤„ç†ä¸€å¤©çš„æ‰€æœ‰å½•éŸ³æ–‡ä»¶ã€‚

        Args:
            date: YYYY-MM-DD
            files: æŒ‰æ—¶é—´æ’åºçš„ WAV æ–‡ä»¶åˆ—è¡¨
        """
        # 1. é€æ–‡ä»¶èµ° pipelineï¼ˆå¤ç”¨ç°æœ‰é€»è¾‘ï¼‰ï¼Œæ”¶é›†æ‰€æœ‰ segment ç»“æœ
        all_results: list[AnalysisResult] = []
        for wav in files:
            results = await self.pipeline.process_without_archive(wav)
            all_results.extend(results)

        # 2. è·¨æ–‡ä»¶åœºæ™¯åˆå¹¶
        merged = self.scene_merger.merge(all_results)

        # 3. ç»Ÿä¸€å½’æ¡£
        entries = self.pipeline.archiver.archive_all(
            merged, archive_date=date, source_file="daily-batch"
        )

        # 4. ç”Ÿæˆæ—¥æŠ¥
        return DailyReport(date=date, files=files, entries=entries, results=merged)
```

**å…³é”®è®¾è®¡**:
- `Pipeline` æ–°å¢ `process_without_archive()` æ–¹æ³•ï¼Œåªè·‘ chunkâ†’ASRâ†’segmentâ†’classifyâ†’analyzeï¼Œä¸å½’æ¡£
- å½’æ¡£ç»Ÿä¸€ç”± `DailyBatchProcessor` åœ¨åˆå¹¶åæ‰§è¡Œ
- æ¯ä¸ª `AnalysisResult` éœ€è¦æºå¸¦æºæ–‡ä»¶ä¿¡æ¯å’Œç»å¯¹æ—¶é—´æˆ³ï¼Œç”¨äºåç»­åˆå¹¶åˆ¤æ–­

### 2. æ–‡ä»¶æ”¶é›†å™¨ â€” ä»æ–‡ä»¶åè§£ææ—¥æœŸ

**ä½ç½®**: `src/audio_journal/batch.py`ï¼ˆåŒæ–‡ä»¶ï¼‰

```python
import re
from datetime import datetime

# åŒ¹é… YYYYMMDDHHMMSS.WAV
_FILENAME_RE = re.compile(r"^(\d{4})(\d{2})(\d{2})(\d{2})(\d{2})(\d{2})\.wav$", re.IGNORECASE)

def parse_recording_time(filename: str) -> datetime | None:
    """ä»æ–‡ä»¶åè§£æå½•éŸ³å¼€å§‹æ—¶é—´ã€‚"""
    m = _FILENAME_RE.match(filename)
    if not m:
        return None
    return datetime(
        int(m[1]), int(m[2]), int(m[3]),
        int(m[4]), int(m[5]), int(m[6])
    )

def collect_files_by_date(inbox: Path) -> dict[str, list[Path]]:
    """æ‰«æ inboxï¼ŒæŒ‰æ—¥æœŸåˆ†ç»„å¹¶æŒ‰æ—¶é—´æ’åºã€‚"""
    groups: dict[str, list[tuple[datetime, Path]]] = {}
    for f in inbox.glob("*.wav"):
        ts = parse_recording_time(f.name)
        if ts is None:
            continue  # è·³è¿‡ä¸ç¬¦åˆå‘½åè§„åˆ™çš„æ–‡ä»¶
        date_str = ts.strftime("%Y-%m-%d")
        groups.setdefault(date_str, []).append((ts, f))

    return {
        date: [p for _, p in sorted(items)]
        for date, items in sorted(groups.items())
    }
```

### 3. Pipeline æ‹†åˆ†ï¼šå¤„ç† vs å½’æ¡£

**æ”¹åŠ¨æ–‡ä»¶**: `src/audio_journal/pipeline.py`

å½“å‰ `Pipeline.process()` æœ«å°¾ç›´æ¥è°ƒç”¨ `self.archiver.archive_all()`ã€‚éœ€è¦æ‹†åˆ†ï¼š

```python
class Pipeline:
    async def process(self, audio_path: str | Path) -> list[AnalysisResult]:
        """å®Œæ•´æµç¨‹ï¼ˆå•æ–‡ä»¶æ¨¡å¼ï¼Œä¿æŒå‘åå…¼å®¹ï¼‰ã€‚"""
        results = await self.process_without_archive(audio_path)
        self.archiver.archive_all(results, source_file=str(Path(audio_path).name))
        return results

    async def process_without_archive(self, audio_path: str | Path) -> list[AnalysisResult]:
        """åªè·‘åˆ†æï¼Œä¸å½’æ¡£ï¼ˆä¾›æ‰¹å¤„ç†è°ƒç”¨ï¼‰ã€‚"""
        src = Path(audio_path)
        run_dir = (self.config.paths.processing / src.stem).resolve()
        chunks_dir = run_dir / "chunks"
        chunks_dir.mkdir(parents=True, exist_ok=True)

        chunks = self.chunker.split(src, chunks_dir)

        all_results: list[AnalysisResult] = []
        for chunk in chunks:
            utterances = self.asr.transcribe(str(chunk.path))
            segments = self.segmenter.segment(utterances, source_file=str(src.name))

            classified = []
            for seg in segments:
                classified.append(await self.classifier.classify(seg))

            for seg in classified:
                if seg.scene == SceneType.MEETING:
                    res = await self.meeting_analyzer.analyze(seg)
                else:
                    res = await self.passthrough_analyzer.analyze(seg)
                all_results.append(res)

        return all_results
```

### 4. SceneMerger â€” è·¨æ–‡ä»¶åœºæ™¯åˆå¹¶

**ä½ç½®**: `src/audio_journal/merger.py`

**åˆå¹¶é€»è¾‘**:
- ç›¸é‚»æ–‡ä»¶çš„æœ«å°¾/å¼€å¤´å¦‚æœæ˜¯åŒä¸€åœºæ™¯ç±»å‹ï¼ˆå¦‚éƒ½æ˜¯ meetingï¼‰ï¼Œä¸”æ—¶é—´é—´éš” < é˜ˆå€¼ï¼ˆå¦‚ 5 åˆ†é’Ÿï¼‰ï¼Œåˆ¤å®šä¸ºåŒä¸€åœºæ™¯
- åˆå¹¶æ—¶æ‹¼æ¥ utterancesã€åˆå¹¶ topics/key_pointsï¼Œé‡æ–°ç”Ÿæˆ summary

```python
class SceneMerger:
    """è·¨æ–‡ä»¶åœºæ™¯åˆå¹¶å™¨ã€‚"""

    def __init__(self, config: AppConfig):
        self.max_gap_seconds = config.batch.merge_gap_seconds  # é»˜è®¤ 300s

    def merge(self, results: list[AnalysisResult]) -> list[AnalysisResult]:
        """åˆå¹¶ç›¸é‚»çš„åŒåœºæ™¯ç»“æœã€‚"""
        if not results:
            return []

        merged: list[AnalysisResult] = [results[0]]
        for current in results[1:]:
            prev = merged[-1]
            if self._should_merge(prev, current):
                merged[-1] = self._do_merge(prev, current)
            else:
                merged.append(current)
        return merged

    def _should_merge(self, a: AnalysisResult, b: AnalysisResult) -> bool:
        """åˆ¤æ–­ä¸¤ä¸ªç»“æœæ˜¯å¦åº”è¯¥åˆå¹¶ã€‚"""
        # åœºæ™¯ç±»å‹å¿…é¡»ç›¸åŒ
        if a.scene != b.scene:
            return False
        # æ—¶é—´é—´éš”æ£€æŸ¥ï¼ˆéœ€è¦ metadata ä¸­çš„æ—¶é—´ä¿¡æ¯ï¼‰
        a_end = a.metadata.get("end_time", 0)
        b_start = b.metadata.get("start_time", 0)
        if b_start - a_end > self.max_gap_seconds:
            return False
        return True

    def _do_merge(self, a: AnalysisResult, b: AnalysisResult) -> AnalysisResult:
        """åˆå¹¶ä¸¤ä¸ªåˆ†æç»“æœã€‚"""
        return AnalysisResult(
            segment_id=a.segment_id,  # ä¿ç•™ç¬¬ä¸€ä¸ªçš„ ID
            scene=a.scene,
            summary=a.summary + "\n" + b.summary,
            key_points=a.key_points + b.key_points,
            action_items=a.action_items + b.action_items,
            participants=list(set(a.participants + b.participants)),
            topics=list(dict.fromkeys(a.topics + b.topics)),  # å»é‡ä¿åº
            raw_text=a.raw_text + "\n\n---\n\n" + b.raw_text,
            metadata={
                **a.metadata,
                "merged_from": a.metadata.get("merged_from", [a.segment_id]) + [b.segment_id],
                "start_time": a.metadata.get("start_time", 0),
                "end_time": b.metadata.get("end_time", 0),
            },
        )
```

### 5. AnalysisResult æ‰©å±• â€” æºå¸¦æ—¶é—´ä¿¡æ¯

**æ”¹åŠ¨æ–‡ä»¶**: `src/audio_journal/models/schemas.py`

```python
class AnalysisResult(BaseModel):
    segment_id: str
    scene: SceneType
    summary: str = ""
    key_points: list[str] = Field(default_factory=list)
    action_items: list[str] = Field(default_factory=list)
    participants: list[str] = Field(default_factory=list)
    topics: list[str] = Field(default_factory=list)
    value_level: str = "normal"
    raw_text: str
    source_file: str = ""                    # â† æ–°å¢ï¼šæº WAV æ–‡ä»¶å
    recording_time: Optional[str] = None     # â† æ–°å¢ï¼šå½•éŸ³å¼€å§‹æ—¶é—´ ISO æ ¼å¼
    metadata: dict[str, Any] = Field(default_factory=dict)
```

Pipeline å¤„ç†æ—¶éœ€è¦å°† `source_file` å’Œä»æ–‡ä»¶åè§£æçš„ `recording_time` å†™å…¥æ¯ä¸ª resultï¼ŒSceneMerger æ®æ­¤åˆ¤æ–­æ—¶é—´é—´éš”ã€‚

### 6. DailyReport â€” æ—¥æŠ¥æ¨¡å‹

**ä½ç½®**: `src/audio_journal/models/schemas.py`

```python
class DailyReport(BaseModel):
    """ä¸€å¤©çš„å¤„ç†æŠ¥å‘Šã€‚"""
    date: str                          # YYYY-MM-DD
    file_count: int                    # å¤„ç†çš„ WAV æ–‡ä»¶æ•°
    total_duration_minutes: float      # æ€»å½•éŸ³æ—¶é•¿ï¼ˆåˆ†é’Ÿï¼‰
    segment_count: int                 # åˆ†æç‰‡æ®µæ•°ï¼ˆåˆå¹¶åï¼‰
    scene_distribution: dict[str, int] # åœºæ™¯åˆ†å¸ƒ {"meeting": 3, "phone": 1}
    entries: list[str]                 # å½’æ¡£æ¡ç›® ID åˆ—è¡¨
```

æ—¥æŠ¥å½’æ¡£åˆ° `data/archive/YYYY-MM-DD/daily-report.md`ï¼Œä½œä¸ºå½“å¤©çš„ç´¢å¼•é¡µã€‚

### 7. é…ç½®æ‰©å±•

**æ”¹åŠ¨æ–‡ä»¶**: `config.yaml` + `src/audio_journal/config.py`

```yaml
batch:
  mode: daily                    # daily | realtimeï¼ˆä¿ç•™å®æ—¶æ¨¡å¼å…¼å®¹ï¼‰
  merge_gap_seconds: 300         # åœºæ™¯åˆå¹¶çš„æœ€å¤§æ—¶é—´é—´éš”ï¼ˆç§’ï¼‰
  auto_move_processed: true      # å¤„ç†å®Œåå°† WAV ç§»åˆ° processed/ ç›®å½•
  processed_dir: ./data/processed
```

```python
class BatchConfig(BaseModel):
    mode: Literal["daily", "realtime"] = "daily"
    merge_gap_seconds: float = 300.0
    auto_move_processed: bool = True
    processed_dir: Path = Path("./data/processed")

class AppConfig(BaseModel):
    # ... ç°æœ‰å­—æ®µ ...
    batch: BatchConfig = Field(default_factory=BatchConfig)
```

### 8. CLI æ”¹é€ 

**æ”¹åŠ¨æ–‡ä»¶**: `src/audio_journal/cli.py`

```python
@main.command()
@click.option("--date", "target_date", type=str, default=None,
              help="å¤„ç†æŒ‡å®šæ—¥æœŸï¼Œæ ¼å¼ YYYY-MM-DDã€‚é»˜è®¤å¤„ç†æ˜¨å¤©ã€‚")
@click.pass_obj
def batch(obj: dict, target_date: str | None) -> None:
    """æ‰¹é‡å¤„ç†æŒ‡å®šæ—¥æœŸçš„æ‰€æœ‰å½•éŸ³ã€‚"""
    cfg: AppConfig = obj["config"]

    if target_date is None:
        target_date = (date.today() - timedelta(days=1)).isoformat()

    files_by_date = collect_files_by_date(cfg.watcher.watch_dir)
    files = files_by_date.get(target_date, [])

    if not files:
        click.echo(f"ğŸ“­ {target_date} æ²¡æœ‰æ‰¾åˆ°å½•éŸ³æ–‡ä»¶")
        return

    click.echo(f"ğŸ™ï¸ å‘ç° {len(files)} ä¸ªå½•éŸ³æ–‡ä»¶ ({target_date})")
    for f in files:
        click.echo(f"  - {f.name}")

    pipe = create_pipeline(cfg)
    processor = DailyBatchProcessor(cfg, pipe)
    report = asyncio.run(processor.process_day(target_date, files))

    click.echo(f"\nâœ… å¤„ç†å®Œæˆ: {report.segment_count} ä¸ªç‰‡æ®µ, {report.file_count} ä¸ªæ–‡ä»¶")

@main.command(name="batch-all")
@click.pass_obj
def batch_all(obj: dict) -> None:
    """å¤„ç† inbox ä¸­æ‰€æœ‰æœªå¤„ç†çš„æ—¥æœŸã€‚"""
    cfg: AppConfig = obj["config"]
    files_by_date = collect_files_by_date(cfg.watcher.watch_dir)

    if not files_by_date:
        click.echo("ğŸ“­ inbox ä¸ºç©º")
        return

    pipe = create_pipeline(cfg)
    processor = DailyBatchProcessor(cfg, pipe)

    for d, files in files_by_date.items():
        click.echo(f"\nğŸ“… å¤„ç† {d} ({len(files)} ä¸ªæ–‡ä»¶)...")
        report = asyncio.run(processor.process_day(d, files))
        click.echo(f"  âœ… {report.segment_count} ä¸ªç‰‡æ®µ")
```

### 9. Watcher æ”¹é€  â€” æ”¶é›†æ¨¡å¼

**æ”¹åŠ¨æ–‡ä»¶**: `src/audio_journal/watcher/file_watcher.py`

å®æ—¶æ¨¡å¼ä¿ç•™ï¼ˆ`batch.mode: realtime`ï¼‰ï¼Œä½†é»˜è®¤æ”¹ä¸ºæ”¶é›†æ¨¡å¼ï¼š

- Watcher ä»ç„¶ç›‘å¬ inboxï¼Œä½†ä¸å†ç«‹å³è§¦å‘ pipeline
- åªè®°å½•æ–°æ–‡ä»¶åˆ° `data/pending.jsonl`ï¼ˆæ–‡ä»¶å + å‘ç°æ—¶é—´ï¼‰
- å®é™…å¤„ç†ç”± `batch` å‘½ä»¤æˆ– cron è§¦å‘

```python
class CollectorHandler(FileSystemEventHandler):
    """æ”¶é›†æ¨¡å¼ï¼šåªè®°å½•æ–°æ–‡ä»¶ï¼Œä¸å¤„ç†ã€‚"""

    def __init__(self, pending_log: Path, stable_seconds: float):
        self.pending_log = pending_log
        self.stable_seconds = stable_seconds

    def on_created(self, event):
        if getattr(event, "is_directory", False):
            return
        p = Path(str(getattr(event, "src_path", "")))
        if p.suffix.lower() != ".wav":
            return
        wait_stable(p, stable_seconds=self.stable_seconds)
        # åªè®°å½•ï¼Œä¸å¤„ç†
        with self.pending_log.open("a") as f:
            f.write(json.dumps({"file": str(p), "discovered": datetime.now().isoformat()}) + "\n")
```

### 10. å·²å¤„ç†æ–‡ä»¶ç®¡ç†

å¤„ç†å®Œæˆåï¼ŒWAV æ–‡ä»¶ç§»åŠ¨åˆ° `data/processed/YYYY-MM-DD/` ç›®å½•ï¼š

```
data/
â”œâ”€â”€ inbox/                    # æ–°å½•éŸ³æ”¾è¿™é‡Œ
â”‚   â”œâ”€â”€ 20260302120546.WAV
â”‚   â””â”€â”€ 20260302132609.WAV
â”œâ”€â”€ processed/                # å¤„ç†å®Œè‡ªåŠ¨ç§»è¿‡æ¥
â”‚   â””â”€â”€ 2026-03-01/
â”‚       â”œâ”€â”€ 20260301120546.WAV
â”‚       â””â”€â”€ 20260301132609.WAV
â”œâ”€â”€ processing/               # ä¸´æ—¶å¤„ç†ç›®å½•ï¼ˆchunk ç­‰ï¼‰
â””â”€â”€ archive/                  # å½’æ¡£ç»“æœ
    â””â”€â”€ 2026-03-01/
        â”œâ”€â”€ daily-report.md
        â”œâ”€â”€ 001-meeting-é¡¹ç›®è¿›åº¦.md
        â”œâ”€â”€ 002-phone-å®¢æˆ·æ²Ÿé€š.md
        â””â”€â”€ index.jsonl
```

è¿™æ ·å¯ä»¥ï¼š
- é€šè¿‡ inbox æ˜¯å¦ä¸ºç©ºåˆ¤æ–­æœ‰æ— å¾…å¤„ç†æ–‡ä»¶
- é¿å…é‡å¤å¤„ç†
- ä¿ç•™åŸå§‹å½•éŸ³ï¼ˆä¸åˆ é™¤ï¼‰

---

## ä¸æ”¹åŠ¨çš„éƒ¨åˆ†

ä»¥ä¸‹æ¨¡å—ä¿æŒä¸å˜ï¼Œç›´æ¥å¤ç”¨ï¼š

| æ¨¡å— | åŸå›  |
|------|------|
| `VADChunker` | å•æ–‡ä»¶åˆ‡åˆ†é€»è¾‘ä¸å˜ |
| `MockASR` / ASR æ¥å£ | è½¬å†™é€»è¾‘ä¸å˜ |
| `SilenceSegmenter` | åˆ†æ®µé€»è¾‘ä¸å˜ |
| `SceneClassifier` | åˆ†ç±»é€»è¾‘ä¸å˜ |
| `MeetingAnalyzer` | åˆ†æé€»è¾‘ä¸å˜ |
| `LocalArchiver` | å½’æ¡£é€»è¾‘ä¸å˜ï¼Œåªæ˜¯è°ƒç”¨æ—¶æœºå˜äº† |
| `JSONLArchiveIndex` | ç´¢å¼•ç»“æ„ä¸å˜ |

---

## å®ç°é¡ºåº

| # | ä»»åŠ¡ | æ”¹åŠ¨èŒƒå›´ | ä¾èµ– |
|---|------|---------|------|
| 1 | `AnalysisResult` æ–°å¢ `source_file` + `recording_time` å­—æ®µ | schemas.py | æ—  |
| 2 | Pipeline æ‹†åˆ† `process_without_archive()` | pipeline.py | #1 |
| 3 | æ–‡ä»¶æ”¶é›†å™¨ `parse_recording_time` + `collect_files_by_date` | batch.py (æ–°) | æ—  |
| 4 | `BatchConfig` é…ç½® | config.py, config.yaml | æ—  |
| 5 | `SceneMerger` å®ç° | merger.py (æ–°) | #1 |
| 6 | `DailyBatchProcessor` + `DailyReport` | batch.py, schemas.py | #2, #3, #5 |
| 7 | CLI `batch` + `batch-all` å‘½ä»¤ | cli.py | #6 |
| 8 | å·²å¤„ç†æ–‡ä»¶ç§»åŠ¨é€»è¾‘ | batch.py | #6 |
| 9 | Watcher æ”¶é›†æ¨¡å¼ | file_watcher.py | #4 |
| 10 | æ—¥æŠ¥ç”Ÿæˆ `daily-report.md` | batch.py | #6 |
| 11 | æµ‹è¯•ç”¨ä¾‹ | tests/ | å…¨éƒ¨ |

**é¢„ä¼°å·¥ä½œé‡**: æ–°å¢ ~400 è¡Œä»£ç ï¼Œæ”¹åŠ¨ ~50 è¡Œç°æœ‰ä»£ç ã€‚

---

## æœªæ¥æ‰©å±•ï¼ˆä¸åœ¨æœ¬æ¬¡èŒƒå›´ï¼‰

- **è·¨æ–‡ä»¶è¯´è¯äººå…³è”** â€” åŸºäºå£°çº¹åµŒå…¥çš„ SpeakerTrackerï¼ˆPhase 3ï¼‰
- **åˆå¹¶åé‡æ–°åˆ†æ** â€” åˆå¹¶çš„ segment é‡æ–°è°ƒç”¨ LLM ç”Ÿæˆæ›´å®Œæ•´çš„ summaryï¼ˆéœ€è¦é¢å¤– API è°ƒç”¨ï¼‰
- **cron è‡ªåŠ¨è§¦å‘** â€” æ¯å¤©å‡Œæ™¨è‡ªåŠ¨æ‰§è¡Œ `batch --date yesterday`
